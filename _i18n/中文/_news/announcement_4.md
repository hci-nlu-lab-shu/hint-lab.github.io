实验室两篇论文被EMNLP2023（The 2023 Conference on Empirical Methods in Natural Language Processing）接收。在王昊老师的指导下，我组陈夏华同学（研三）主要参与一篇长文（Regular）论文被NLP领域顶级会议EMNLP2023主会接收，同时我组另一位王庆旋（研二）、李越（本科生）、王常青（研二）同学参与的一篇长文论文被EMNLP2023-findings接收。以上研究为本实验室与上海交通大学、日本京都大学开展的联合研究，主要探讨了人类视觉富文档认知过程和机理，为进一步构建强人工智能系统提供了一定的理论支撑。

EMNLP是人工智能/NLP方向的顶级会议之一（CAAI—A类、CCF-B类）作为国际语言学会(ACL)下属的SIGDAT小组主办的自然语言处理领域的顶级国际会议，在计算语言学类别下影响力排名全球第二，以上两篇论文的工作为为我校首次在该会议接收的论文。EMNLP2023将于2023年12月6日至12月10日在新加坡召开。

***

会议论文信息如下：

**标题：** 
Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning


**简介：** 
从视觉丰富的表格类文档（VFD）中提取属于预定义类别的有意义实体是一项具有挑战性的任务。视觉和布局特征（如字体、背景、颜色以及边界框的位置和大小）为识别同类实体提供了重要线索。在本文中，我们提出了一种新颖的视觉非对称协同学习（VANCL）方法，通过加入颜色先验来增强模型捕捉细粒度视觉和布局特征的能力，从而解决了上述局限性。基准数据集上的实验结果表明，我们的方法大大优于强大的LayoutLM系列基线，证明了我们方法的有效性。此外，我们还研究了不同颜色方案对我们方法的影响，为优化模型性能提供了启示。我们相信，我们的工作将对未来的多模态信息提取研究有所启发。


**标题：** 
DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading

**简介：** 
各领域对视觉丰富的文档的使用，催生了对能够像人类一样阅读和理解文档的文档人工智能模型的需求，这需要克服技术、语言和认知障碍。遗憾的是，缺乏合适的数据集严重阻碍了这一领域的发展。为了解决这个问题，我们引入了 DocTrack，这是一个视觉丰富的文档数据集，利用眼动跟踪技术真正与人类眼动信息保持一致。该数据集可用于研究上述挑战。此外，我们还探讨了人类阅读顺序对文档理解任务的影响，并研究了如果机器按照与人类相同的顺序阅读会发生什么。我们的研究结果表明，虽然文档人工智能模型已经取得了重大进展，但要像人类一样准确、连续、灵活地阅读视觉内容更丰富的文档，还有很长的路要走。这些发现对未来文档智能的研究和发展具有潜在的影响。